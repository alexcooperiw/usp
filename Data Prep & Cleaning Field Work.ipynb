{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AA - Data Prep & Cleaning Field Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will aggregate, prepare and clean data.  You are required to enter code code in the blocks beginning with _#TASK:_ \n",
    "\n",
    "__Description:__\n",
    "* <a href = '#sec1'>Preliminary</a>\n",
    "* <a href = '#sec2'>Exercise 1: Data preparation</a>\n",
    "    * <a href = '#sec21'>1a: Basic quality checks</a>\n",
    "    * <a href = '#sec22'>1b: Join data sources</a>\n",
    "    * <a href = '#sec23'>1c: Encode missing values</a>\n",
    "    * <a href = '#sec25'>1d: Format corrections </a>\n",
    "    * <a href = '#sec24'>1e: Encode values consistently</a>\n",
    "* <a href = '#sec3'>Data cleaning</a>\n",
    "    * <a href = '#sec31'>Exercise 2: Handling missing values </a>\n",
    "    * <a href = '#sec43'>Exercise 3: Eliminating outliers </a>\n",
    "    * <a href = '#sec42'>Exercise 4: Redundancy / Inconsistency </a>\n",
    "* <a href = '#sec6'>4. Save the prepared data set</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id='sec1'></a>\n",
    "# Preliminary\n",
    "\n",
    "### Import required packages and change directory \n",
    "__Required packages:__ \n",
    "* os \n",
    "* numpy\n",
    "* pandas  \n",
    "* matplotlib\n",
    "* seaborn\n",
    "\n",
    "<br>\n",
    "You can import packages using the \n",
    "\n",
    "    import\n",
    "\n",
    "command. In this module, we are working primarily with **pandas**, which is the standard package in Python for data manipulation.\n",
    "\n",
    "For visualization purposes, we will use **matplotlib** (the standard Python plotting library) as well as **seaborn**. *Matplotlib* is very versatile and customizable, but can require a few lines of code. *Seaborn* is more convenient for the main chart types, but is less customizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#set pandas to display 50 table columns by default\n",
    "pd.set_option('display.max.columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv(\"bankingchurn_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id='sec2'></a>\n",
    "# Exercise 1: Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1a: Basic quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a start, we should take a look at the data before we start working with it. Some basic quality checks include:\n",
    "\n",
    "- Review the first and last rows\n",
    "- Encode of missing values\n",
    "- Format corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec21'></a>\n",
    "#### Review the first and last rows\n",
    "Printing the first rows gives us a quick overview whether the data was loaded in properly. Do you already notice something strange?\n",
    "\n",
    "     head(): Returns the first rows\n",
    "     tail(): Returns the last rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Show the first 10 rows of churn_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Show the last 5 rows of churn_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data types. \n",
    "\n",
    "     dtypes: displays the data types for each column\n",
    "     info(): displays information about the data frame, including data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TASK: Show the data types of churn_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: List any observations that seem perculiar regarding the data types shown above.  No need for code here, plain text is fine :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1b: Join data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have managed to obtain data pertaining to customer withdrawal values and would like to incorporate this into our master data frame.  To do this we need to:\n",
    "\n",
    "- Load the withdrawal data in \"cash_withdrawals_data.csv\"\n",
    "- Identify a column in each of our data frames to join the data on\n",
    "- Join the cash withdrawal data into the master churn_df dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load cash_withdrawals_data.csv\n",
    "cash_withdraw_df = pd.read_csv(\"cash_withdrawals_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Show the top 10 rows of the cash withdrawals data and try to identify which column you should join on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Merge the withdrawal data into churn_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Show the first 10 rows of churn_df and check that the merge worked. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you notice any issues as a result of the join?  List them below.\n",
    "#TASK: List potential issues introduced by the join\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec22'></a>\n",
    "### Exercise 1c: Encode missing values\n",
    "In the first rows we see that missing values in the column _contract_end_ are encoded as '-'. This can be problematic because Python won't be able to identify the correct data type of these columns. Other commonly used values for missing values are '999' or 'NA'. \n",
    "<br>\n",
    "\n",
    "Let's check which columns have '-' as missing values.\n",
    "\n",
    "        isin(): checks whether an element is contained in values\n",
    "        replace(): replaces a given value with another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Check which other columns contain \"-\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to solve this, the most elegant is to replace\n",
    "\"-\" with \"NaN\". For this we can use the _.NaN_ functionality of the numpy package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: For each of the columns which contain \"-\", replace \"-\" with NaN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "<a id='sec24'></a>\n",
    "### Exercise 1d: Format corrections\n",
    "Let's look at the data types again and try fix the observations we made earlier regarding column types. \n",
    "\n",
    "     dtypes: displays the data types for each column\n",
    "     info(): displays information about the data frame, including data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert to Datetime:__ We see that several date columns are of type 'object', which typically represents text. Python has a specific data type **datetime** which is designed to work with time-based data. Let's change the data type of that column. Let's convert them to date format. \n",
    "\n",
    "    to_datetime(): Convert argument to datetime\n",
    "    to_datetime(dayfirst=True): Convert argument to datetime for date formats where the day is the first entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Convert \"data_of_birth\", \"contract_start\" and \"contract_end\" columns from type 'object' to 'datetime'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we check how the data looks like now, we notice that the formatting has changed to 'datetime64'. It is now a proper time-based data type, which we could manipulate easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert to Category__: Some columns are categorical variables, but stored as 'object'. Let's convert them accordingly.\n",
    "\n",
    "    astype('category'): converts to categorical datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK:  Convert \"gender\", \"profession\", \"ZIP\" and \"segment\" columns from type 'object' to 'category'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking in the data set, we see that all binary variables are stored as 'object'. Luckily, they all contain the string 'flag'. Using the str method, we can convert them all together. \n",
    "\n",
    "    str.contains(): Test if pattern or regex is contained within a string or a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter out all binary variables\n",
    "binary_var = churn_df.columns[churn_df.columns.str.contains('flag')]\n",
    "binary_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all binary variables to categorical type\n",
    "for col_name in binary_var:\n",
    "    churn_df[col_name] = churn_df[col_name].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert to Numeric:__ Some columns should of type 'numeric'. Let's convert them accordingly.\n",
    "\n",
    "    to_numeric(): converts to numeric datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Convert \"credit_rating\" column to numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our results\n",
    "churn_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec23'></a>\n",
    "### Exercise 1e: Encode values consistently\n",
    "We notice that all binary variables (taking on exactly two values) are encoded with 0 and 1, except for insurance_house_flag (1, 2)). Let's correct it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df['insurance_house_flag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Ensure that \"insurance_house_flag\" is either 0 or 1, instead of 1 or 2.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df['insurance_house_flag'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "<a id='sec4'></a>\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec43'></a>\n",
    "### Exercise 2: Handling missing values\n",
    "There are missing values, which most algorithms cannot deal with \n",
    "(meaning the information from the column is lost).\n",
    "\n",
    "We have to decide what we do with missing values. Some options are: \n",
    " - Exclude variables with NAs (e.g. using a threshold)\n",
    " - Removing rows with NAs \n",
    " - Consider NAs as a separate catogory\n",
    " - Impute missing values with an aggregated value (e.g. mean, median, min, max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify rows with missing values\n",
    "     \n",
    "     isna(): Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK:  Identify the number of rows with missing values in each column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What do you suggest for the four variables with missing values?__\n",
    "\n",
    "Remember what we did for _contract_end_ , _profession_ and _credit_rating_ earlier during the workshop on Day 2?  What do we want to do now about missing values in _cash_withdrawals_value_ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK:  List out your chosen action for handling NAs for each of the 4 variables with missing values, i.e. Keep variable as is, remove variable, impute with median, encode missing values as separate category and give your reasons\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Show the count of all the unique values contained in \"profession\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Consider NAs in \"profession\" as a separate category (\"unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK Impute NAs in \"credit_rating\" with the median of the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Impute NAs in \"cash_withdrawals_value\" with the mean of the relevant segment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec41'></a>\n",
    "## Exercise 3: Eliminate outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an initial overview using describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Generate the key metrics (e.g. mean, median, max, min, etc.) of 'cash_withdrawals_value' column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be some very obvious outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform visual inspection using a boxplot or a histogram.\n",
    "\n",
    "    plt.hist()\n",
    "    plt.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Create the histogram of cash_withdrawals_value with 30 bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Create the boxplot of cash_withdrawals_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Identify a threshold value you would use to remove the outliers, and then remove the outliers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Regenerate the histogram with outliers removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Regenerate the box plot with outliers removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there still outliers?  What would your next steps be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK:  Outline your observations and next steps below regarding the cash_withdrawals_value column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec42'></a>\n",
    "### Exercise 4: Redundancy / Inconsistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling duplicates\n",
    "We should always delete duplicate observations as they don't provide any new information\n",
    "\n",
    "     duplicated(): Returns a series of TRUE/FALSE denoting duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Identify duplicated rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Watch out for row indices!\n",
    "When looking for duplicate rows, we should be wary of row indices.  It would appear that we do not have any duplicate rows, but try dropping the column _customer_id_ and checking for duplicates again.\n",
    "\n",
    "     drop(columns = 'column'): Drops 'column' from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Remove column \"customer_id\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Identify duplicated rows again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to drop duplicate rows is to use drop_duplicates()\n",
    "\n",
    "     drop_duplicates(): removes duplicated rows from a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Delete duplicated rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "<a id='sec6'></a>\n",
    "## 6. Save the cleaned data set\n",
    "Save Python object as a file using 'pickle'. You also save the file as a CSV.\n",
    "\n",
    "    to_pickle(): Pickle (serialize) Python object to file\n",
    "    to_csv(): Write Python object to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.to_pickle(\"churn_for_engineering.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
